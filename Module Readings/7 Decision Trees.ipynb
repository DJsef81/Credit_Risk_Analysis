{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "17.7.1 Overview of Decision Trees\n",
    "\n",
    "You're becoming more and more comfortable with supervised learning, and Jill suggests that you look into decision trees. The basic idea behind decision trees is simple. Furthermore, decision trees can be combined into even more powerful classifiers. In fact, decision trees are the basis of many models that win machine learning competitions."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As you may have guessed, decision trees are used in decision analysis, like determining if a coin flip will be heads or tails, or whether a loan application is approved. \n",
    "\n",
    "In short, decision trees encode a series of true/false questions that are represented by a series of if/else statements. Decision trees are one of the most interpretable models, as they provide a clear representation of how the model works.\n",
    "\n",
    "Decision trees are natural ways in which you can classify or label objects by asking a series of questions designed to zero in on the true answer. However, decision trees can become very complex and very deep, depending on how many questions have to be answered. Deep and complex trees tend to overfit to the data and do not generalize well."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The following example, a pet picker decision tree, covers some key concepts:\n",
    "\n",
    "At the top of this decision tree is the root node or the parent node: \"Pet Picker: Do you travel?\" The root node represents the entire population. \n",
    "\n",
    "This node gets divided into two or more homogeneous sets in our decision tree to answer the question, \"Do you travel?\" Each answer, \"Yes\" or \"No,\" is a branch or subsection of the tree. When we divide a node into two or more subnodes, it's called splitting.\n",
    "\n",
    "When we split the root node into two subnodes, they are called child nodes. Our child nodes include two questions: \"Are you gone more than one week per month?\" and \"Do you like to dress up your pets?\" \n",
    "\n",
    "These child nodes are split again into subnodes—the images in the decision tree—making each child node a decision node. The four images do not split further and are referred to as a leaf or terminal node.\n",
    "\n",
    "Our pet picker decision tree depth is not that deep because, at two, it has a low number of decision nodes one encounters before making a decision: \"Are you gone more than one week per month?\" and \"Do you like to dress up your pets?\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Decision trees, as you have seen, are like a series of if-else statements. But how does a decision tree determine which if-else statement to use first? In other words, how does it decide on the root node?\n",
    "\n",
    "To understand this process, let's imagine a scenario in which we use multiple factors, such as body mass index (BMI) over 30, and a history of high blood pressure, to predict whether a patient has diabetes. \n",
    "\n",
    "There are two candidates for the root node in this example: BMI and blood pressure. The classification that creates the best split becomes the root node.\n",
    "\n",
    "For example, let's say that a history of hypertension best predicts whether a person will be diabetic. In that case, it becomes the root node:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The same idea holds when the variables are continuous rather than discrete. Let's say that we're dealing with the same two variables, BMI and blood pressure, to predict diabetes. The decision tree looks for the best value along each axis to split the diabetics from non-diabetics:\n",
    "\n",
    "Looking at this image, we can see that a vertical line along the x-axis (blood pressure) would best split the dataset into diabetics and non-diabetics, resulting in the fewest errors. In other words, a specific blood pressure value can be used to split the dataset into diabetic and non diabetic groups:\n",
    "\n",
    "The same process is repeated to create subsequent child nodes. In this example, a horizontal line that represents a particular BMI will become the next node:\n",
    "\n",
    "(see photos in Supervised Machine Learning Folder)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "17.7.2 Predict Loan Application Approval\n",
    "\n",
    "Now that you have learned how decision trees work, it's time to look at using a decision tree model in practice. You will first perform the data preprocessing steps.\n",
    "\n",
    "Let's try to predict loan application approvals using a decision tree on the loans_data_encoded.csv data we previously used to encode the dataset. Feel free to begin a new notebook, or to follow along the provided notebook.\n",
    "\n",
    "CHECK decision-trees doc. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In Jupyter Notebook, import the following dependencies:\n",
    "\n",
    "# Initial imports\n",
    "import pandas as pd\n",
    "from path import Path\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next, read in your saved loans_data_encoded.csv file.\n",
    "\n",
    "# Loading data\n",
    "file_path = Path(\"../Resources/loans_data_encoded.csv\")\n",
    "df_loans = pd.read_csv(file_path)\n",
    "df_loans.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Our goal is to predict if a loan application is worthy of approval based on information we have in our df_loans DataFrame. To do this, we'll have to split our dataset into features (or inputs) and target (or outputs). \n",
    "\n",
    "The features set, X, will be a copy of the df_loansDataFrame without the badcolumn. These features are all the variables that help determine whether a loan application should be denied.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recall that X is the input data and y is the output data.\n",
    "\n",
    "# Define the features set.\n",
    "X = df_loans.copy()\n",
    "X = X.drop(\"bad\", axis=1)\n",
    "X.head()\n",
    "\n",
    "The output from the following code block will give us the following features set."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The target set is the bad column, indicating whether or not a loan application is good (0) or bad (1). Run the following code to generate the target set data.\n",
    "\n",
    "# Define the target set.\n",
    "y = df_loans[\"bad\"].values\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A preview of the target set indicates five good (loan worthy) applications.\n",
    "\n",
    "array([0, 0, 0, 0, 0])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Split the Data Into Training and Testing Sets\n",
    "\n",
    "To train and validate our model, we'll need to split the features and target sets into training and testing sets. This will help determine the relationships between each feature in the features training set and the target training set, which we'll use to determine the validity of our model using the features and target testing sets.\n",
    "\n",
    "In Jupyter Notebook, add the following code that will split our data into training and testing sets.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "When the train_test_split() function is executed, our data is split into a specific proportion of the original data sets. By default, our training and testing data sets are 75% and 25%, respectively, of the original data. Using the following code, we can see the data's 75-25 split.\n",
    "\n",
    "# Determine the shape of our training and testing sets.\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The output from running the code above shows that the X_train and y_train is 75% of 500 and that the X_test and y_test are 25%.\n",
    "\n",
    "(375, 10)\n",
    "(125, 10)\n",
    "(375, 1)\n",
    "(125, 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can manually specify the desired split with the train_size parameter.\n",
    "\n",
    "# Splitting into Train and Test sets into an 80/20 split.\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, random_state=78, train_size=0.80)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To see that the shape of our training and testing sets is a 80-20 split, we run the following code.\n",
    "\n",
    "# Determine the shape of our training and testing sets.\n",
    "print(X_train2.shape)\n",
    "print(X_test2.shape)\n",
    "print(y_train2.shape)\n",
    "print(y_test2.shape)\n",
    "\n",
    "The output from this code will give the following results.\n",
    "\n",
    "(400, 10)\n",
    "(100, 10)\n",
    "(400, 1)\n",
    "(100, 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Scale the Training and Testing Data\n",
    "\n",
    "Now that we have split our data into training and testing sets, we can scale the data using Scikit-learn's StandardScaler.\n",
    "\n",
    "The standard scaler standardizes the data. Which means that each feature will be rescaled so that its mean is 0 and its standard deviation is 1.\n",
    "\n",
    "Typically, models that compute distances between data points, such as SVM, require scaled data. Although decision trees don't require scaling the data, it can be helpful when comparing the performances of different models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To scale our data, we'll use the StandardScaler as before and fit the instance, scaler, with the training data and then scale the features with the transform() method:\n",
    "\n",
    "# Creating a StandardScaler instance.\n",
    "scaler = StandardScaler()\n",
    "# Fitting the Standard Scaler with the training data.\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scaling the data.\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Which of the following should go in the blank spaces below to compute the mean and the standard deviation of the first column of the scaled data?\n",
    "\n",
    "Import numpy as np\n",
    "np.mean(X_train_scaled___)\n",
    "np.mean(X_test_scaled___)\n",
    "np.std(X_train_scaled___)\n",
    "np.std(X_test_scaled___)\n",
    "\n",
    "[:,0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "17.7.3 Make Predictions and Evaluate Results\n",
    "\n",
    "Now that you have preprocessed your dataset, you can now turn your attention to using the decision tree model to make predictions and evaluate the results."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Fit the Decision Tree Model\n",
    "\n",
    "After scaling the features data, the decision tree model can be created and trained. First, we create the decision tree classifier instance and then we train or fit the \"model\" with the scaled training data.\n",
    "\n",
    "Add and run the following code block to create the decision tree instance and fit the model:\n",
    "\n",
    "# Creating the decision tree classifier instance.\n",
    "model = tree.DecisionTreeClassifier()\n",
    "# Fitting the model.\n",
    "model = model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Make Predictions Using the Testing Data\n",
    "\n",
    "After fitting the model, we can run the following code to make predictions using the scaled testing data:\n",
    "\n",
    "# Making predictions using the testing data.\n",
    "predictions = model.predict(X_test_scaled)\n",
    "\n",
    "The output from this code will be an array of 125 predictions with either a 1 for a bad loan application or a 0 for a good, or approved, loan application.\n",
    "\n",
    "Your predictions array may look different if you don't use the same seeding in the random_state parameter."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Evaluate the Model\n",
    "\n",
    "Finally, we'll determine how well our model classifies loan applications. First, we need to use a confusion matrix.\n",
    "\n",
    "The following code block creates the confusion_matrix using the y_test and the predictions that we just calculated and adds the confusion_matrix array to a DataFrame:\n",
    "\n",
    "# Calculating the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix.\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The results show that:\n",
    "\n",
    "Out of 84 good loan applications (Actual 0), 50 were predicted to be good (Predicted 0), which we call true positives.\n",
    "Out of 84 good loan applications (Actual 0), 34 were predicted to be bad (Predicted 1), which are considered false negatives.\n",
    "Out of 41 bad loan applications (Actual 1), 22 were predicted to be good (Predicted 0) and are considered false positives.\n",
    "Out of 41 bad loan applications (Actual 1), 19 were predicted to be bad (Predicted 1) and are considered true negatives.\n",
    "\n",
    "We can add these terms to the confusion matrix and add the row and column totals to get the following table:\n",
    "\n",
    "\n",
    "n = 125\n",
    "               Predicted Good        Predicted Bad\n",
    "Actual Good           TP = 50              FN = 34    - 84 \n",
    "Actual Bad.           FP = 22.             TN = 19    - 41\n",
    "                           72.                  53  \n",
    "\n",
    "\n",
    "\n",
    "What is the recall (sensitivity) for bad loans (Actual 1)?\n",
    "There are a total of 41 (22 + 19) bad loans. Nineteen out of 41 were classified as bad loans. Therefore, the recall is 0.46 or 46%."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next, we can determine the accuracy, or how often the classifier is correct with the model, by running the following code:\n",
    "\n",
    "# Calculating the accuracy score.\n",
    "acc_score = accuracy_score(y_test, predictions)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next, we can determine the accuracy, or how often the classifier is correct with the model, by running the following code:\n",
    "\n",
    "# Calculating the accuracy score.\n",
    "acc_score = accuracy_score(y_test, predictions)\n",
    "\n",
    "The accuracy of our model is 0.552, which can also be calculated as follows:\n",
    "\n",
    "(True Positives (TP) + True Negatives (TN)) / Total = (50 + 19)/125 = 0.552"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lastly, we can print out the above results along with the classification report, which will give us the precision, recall, F1 score, and support for the two classes.\n",
    "\n",
    "# Displaying results\n",
    "print(\"Confusion Matrix\")\n",
    "display(cm_df)\n",
    "print(f\"Accuracy Score : {acc_score}\")\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let's go over the results in the classification report:\n",
    "\n",
    "Precision: Precision is the measure of how reliable a positive classification is. From our results, the precision for the good loan applications can be determined by the ratio TP/(TP + FP), which is 50/(50 + 22) = 0.69. \n",
    "\n",
    "The precision for the bad loan applications can be determined as follows: 19/(19 + 34) = 0.358. A low precision is indicative of a large number of false positives—of the 53 loan applications we predicted to be bad applications, 34 were actually good loan applications.\n",
    "\n",
    "Recall: Recall is the ability of the classifier to find all the positive samples. It can be determined by the ratio: TP/(TP + FN), or 50/(50 + 34) = 0.595 for the good loans and 19/(19 + 22) = 0.463 for the bad loans. A low recall is indicative of a large number of false negatives.\n",
    "\n",
    "F1 score: F1 score is a weighted average of the true positive rate (recall) and precision, where the best score is 1.0 and the worst is 0.0.\n",
    "\n",
    "Support: Support is the number of actual occurrences of the class in the specified dataset. For our results, there are 84 actual occurrences for the good loans and 41 actual occurrences for bad loans.\n",
    "\n",
    "In summary, this model may not be the best one for preventing fraudulent loan applications because the model's accuracy, 0.552, is low, and the precision and recall are not good enough to state that the model will be good at classifying fraudulent loan applications. \n",
    "\n",
    "Modeling is an iterative process: you may need more data, more cleaning, another model parameter, or a different model. It's also important to have a goal that's been agreed upon, so that you know when the model is good enough.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

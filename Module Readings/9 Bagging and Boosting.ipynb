{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "17.9.1 Bootstrap Aggregation\n",
    "\n",
    "Bootstrap aggregation, or \"bagging,\" is an ensemble learning technique that combines weak learners into a strong learner. In fact, you have already seen a model that uses bootstrap aggregation as part of its algorithm: the random forest model.\n",
    "\n",
    "Jill reminds you that decision trees are prone to overfitting, meaning that the algorithm's predictions are excessively tailored to the specific dataset. When there's overfitting, a model's performance will suffer when it encounters a new dataset. One way to try to overcome this problem is with bootstrap aggregation. Let's look at how it works in more detail."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Bootstrapping\n",
    "\n",
    "Bootstrapping is a sampling technique in which samples are randomly selected, then returned to the general pool and replaced, or put back into the general pool. As a concrete example, picture your dataset as a bag containing five wooden blocks, each labeled with the letter A, B, C, D, or E.\n",
    "\n",
    "Imagine that you draw samples of the dataset from this bag three times. Since each sampling is the same size as the original dataset, you must draw five blocks for each sample. To do so, you grab a wooden block randomly from the bag, and after noting which block you drew, you replace it, meaning that you put it back into the bag. Because you return the block to the bag, it's possible to draw the same block again in the next draw. You repeat the process until you have a sample whose size is the same as the original dataset. The result might appear as follows:\n",
    "\n",
    "Sample 1: A, A, A, B, D\n",
    "\n",
    "Sample 2: A, B, B, C, E\n",
    "\n",
    "Sample 3: B, C, D, D, E\n",
    "\n",
    "In our example above, each sample contains multiple occurrences of the same block. Sample 1 drew the letter A three times, Sample 2 drew the letter B twice, and Sample 3 drew the letter D twice. In other words, each observation (letter) may occur repeatedly in any given sample. In real life, this means that if your dataset were a Pandas DataFrame, a given row may occur multiple times in a sample.\n",
    "\n",
    "In summary, bootstrapping is simply a sampling technique with which a number of samples are made, and in which an observation can occur multiple times.s"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Aggregation\n",
    "\n",
    "In the aggregation step, different classifiers are run, using the samples drawn in the bootstrapping stage. Each classifier is run independently of the others, and all the results are aggregated via a voting process. Each classifier will vote for a label (a prediction). The final prediction is the one with the most votes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "17.9.2 Boosting\n",
    "\n",
    "Boosting is another technique to combine weak learners into a strong learner. However, there is a major difference between bagging and boosting. In bagging, as you have seen, multiple weak learners are combined at the same time to arrive at a combined result.\n",
    "\n",
    "In boosting, however, the weak learners are not combined at the same time. Instead, they are used sequentially, as one model learns from the mistakes of the previous model.\n",
    "\n",
    "Jill assures you that learning this ensemble learning technique will be worth your time. After all, many machine learning competitions have been won with this powerful technique."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Adaptive Boosting\n",
    "\n",
    "The idea behind Adaptive Boosting, called AdaBoost, is easy to understand. In AdaBoost, a model is trained then evaluated. After evaluating the errors of the first model, another model is trained. This time, however, the model gives extra weight to the errors from the previous model. The purpose of this weighting is to minimize similar errors in subsequent models. Then, the errors from the second model are given extra weight for the third model. This process is repeated until the error rate is minimized:\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Gradient Boosting\n",
    "\n",
    "Gradient boosting, like AdaBoost, is an ensemble method that works sequentially. In contrast to AdaBoost, gradient boosting does not seek to minimize errors by adjusting the weight of the errors. Instead, it follows this process:.\n",
    "\n",
    "1. A small tree (called a stump) is added to the model, and the errors are evaluated.\n",
    "\n",
    "2. A second stump is added to the first and attempts to minimize the errors from the first stump. These errors are called pseudo-residuals.\n",
    "\n",
    "3. A third stump is added to the first two and attempts to minimize the pseudo-residuals from the previous two.\n",
    "\n",
    "4. The process is repeated until the errors are minimized as much as possible, or until a specified number of repetitions has been reached:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In gradient boosting, the learning rate refers to how aggressively pseudo-residuals are corrected during each iteration. In general, it is preferable to begin with a lower learning rate and, if necessary, adjust the rate upward.\n",
    "\n",
    "Gradient boosting is a powerful technique that is often used in machine learning competitions."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "17.9.3 Boosting in Practice\n",
    "\n",
    "Armed with an understanding of boosting, you're excited to put this technique into practice. Jill encourages you to dive into gradient boosting. Your next step is to apply your knowledge to Python code."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let's look at an example of using a gradient boosted tree model to enhance the performance of weak learners by combining them into an ensemble.\n",
    "\n",
    "Start by downloading the files you'll need for this section gradient boosting tree"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Open gradient_boosted_tree.ipynb, or if you like, create a new notebook in the same location, and place the following code.\n",
    "\n",
    "import pandas as pd\n",
    "from path import Path\n",
    "file_path = Path(\"../Resources/loans_data_encoded.csv\")\n",
    "loans_df = pd.read_csv(file_path)\n",
    "loans_df.head()\n",
    "\n",
    "A preview of the DataFrame reveals that the dataset again contains information on loan applications. The bad column is the target column, with 0 indicating a good loan application and 1 indicating a bad loan application."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Then, separate the feature columns from the target column.\n",
    "\n",
    "X = loans_df.copy()\n",
    "X = X.drop(\"bad\", axis=1)\n",
    "y = loans_df[\"bad\"].values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next, split the dataset into training and testing sets. Again, the random_state argument is optional.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "   y, random_state=1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The data is scaled in the next step. Scaling is typically necessary when using models that calculate distances between data points, such as SVM. While not strictly required for tree-based models, it can be a good idea to scale the data, especially when comparing the performances of different models.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the next step, a for loop is used to identify the learning rate that yields the best performance.\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "for learning_rate in learning_rates:\n",
    "   classifier = GradientBoostingClassifier(n_estimators=20,\n",
    "   learning_rate=learning_rate,\n",
    "   max_features=5,\n",
    "   max_depth=3,\n",
    "   random_state=0)\n",
    "   classifier.fit(X_train_scaled, y_train.ravel)\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The GradientBoostingClassifier includes the following:\n",
    "\n",
    "- An array called learning_rates is manually created and contains a range of values.\n",
    "\n",
    "- For each learning rate value, a GradientBoostingClassifier model is instantiated.\n",
    "\n",
    "- The max_depth argument refers to the size of the decision tree stumps used in gradient boosting.\n",
    "\n",
    "- The n_estimators argument refers to the number of trees used.\n",
    "\n",
    "- The n_estimators, max_features, and max_depth parameters are fixed at the defined values. These, like the learning rate, can be optimized, but we'll stick to the default values used in the example above.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "During each iteration of the for loop, the accuracy scores of the training and testing sets are also printed for each learning rate.\n",
    "\n",
    "   print(\"Learning rate: \", learning_rate)\n",
    "   print(\"Accuracy score (training): {0:.3f}\".format(\n",
    "       classifier.score(\n",
    "           X_train_scaled,\n",
    "           y_train)))\n",
    "   print(\"Accuracy score (validation): {0:.3f}\".format(\n",
    "       classifier.score(\n",
    "           X_test_scaled,\n",
    "           y_test)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Previously, we used Scikit-learn's accuracy_score module to validate a model. The method used here is classifier.score(), which yields the same result.\n",
    "\n",
    "Of the learning rates used, 0.5 yields the best accuracy score for the testing set and a high accuracy score for the training set. This is the value we'll implement in the final model. Also, note that the testing accuracy is more important here than the training accuracy."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A model that performs well on the training set but poorly on the testing set is said to be \"overfit.\" Overfitting is akin to memorizing the answers to an exam: It will help on that particular exam, but not on any others. In other words, overfitting occurs when a model gives undue importance to patterns within a particular dataset that are not found in other, similar datasets. Instead of learning a general pattern that can be applied to other similar datasets, it learns the patterns specific to one dataset."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using the learning_rate value obtained from the for loop, we instantiate a model, train it, then create predictions.\n",
    "\n",
    "classifier = GradientBoostingClassifier(n_estimators=20,\n",
    "   learning_rate=0.5, max_features=5, max_depth=3, random_state=0)\n",
    "\n",
    "classifier.fit(X_train_scaled, y_train)\n",
    "predictions = classifier.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Having created predictions with the gradient boosted tree model, we can assess the model's performance. This time, the accuracy_score() method is used.\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "acc_score = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy Score : {acc_score}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Predictably, the accuracy_score() method returns the same score as that of the classifier.score()method.\n",
    "\n",
    "Accuracy Score: 0.56\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next, we generate a confusion_matrix of the results.\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "cm_df = pd.DataFrame(\n",
    "   cm, index=[\"Actual 0\", \"Actual 1\"],\n",
    "   columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    ")\n",
    "display(cm_df)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Finally, we can generate a classification report to evaluate the precision, recall, and F1 scores.\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SKILL DRILL\n",
    "How would you interpret the results of this classification report?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

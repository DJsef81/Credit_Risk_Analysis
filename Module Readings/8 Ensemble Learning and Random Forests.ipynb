{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "17.8.1 Overview of Ensemble Learning\n",
    "\n",
    "As you and Jill discuss ways to improve a model's performance, she brings up ensemble learning. Ensemble learning builds on the idea that two is better than one. A single tree may be prone to errors, but many of them can be combined to form a stronger model. A random forest model, for example, combines many decision trees into a forest of trees."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The concept of ensemble learning is the process of combining multiple models, like decision tree algorithms, to help improve the accuracy and robustness, as well as decrease variance of the model, and therefore increase the overall performance of the model."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Weak Learners\n",
    "\n",
    "Some algorithms are weak learners. Weak learners tend to have very few branches on the decision tree. A single weak learner will make inaccurate and imprecise predictions because they are poor at learning adequately as result of limited data, like too few features, or using data points that can't be classified.\n",
    "\n",
    "Weak learners shouldn't be considered unworthy. Weak learners are valuable because there are models that can combine many weak learners to create a more accurate and robust prediction engine. When we combine weak learners, together they can perform just as well as any strong learner.\n",
    "\n",
    "Our loan application prediction algorithm can be considered a moderate to weak learner because it was not good at classifying bad loan applications. However, if we combine this decision tree with more decision trees—all using different training and testing sets—then the prediction may be more accurate.\n",
    "\n",
    "We can combine weak learners using a specific algorithm, like Random Forests, GradientBoostedTree, and XGBoost. We will learn about Random Forests next, and cover gradient boosting later in this module."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Random Forests\n",
    "\n",
    "Instead of having a single, complex tree like the ones created by decision trees, a random forest algorithm will sample the data and build several smaller, simpler decision trees. Each tree is simpler because it is built from a random subset of features:\n",
    "\n",
    "These simple trees are weak learners because they are created by randomly sampling the data and creating a decision tree for only that small portion of data. And since they are trained on a small piece of the original data, they are only slightly better than a random guess. However, many slightly better than average small decision trees can be combined to create a strong learner, which has much better decision making power.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Random forest algorithms are beneficial because they:\n",
    "\n",
    "- Are robust against overfitting as all of those weak learners are trained on different pieces of the data.\n",
    "\n",
    "- Can be used to rank the importance of input variables in a natural way.\n",
    "\n",
    "- Can handle thousands of input variables without variable deletion.\n",
    "\n",
    "- Are robust to outliers and nonlinear data.\n",
    "\n",
    "- Run efficiently on large datasets.\n",
    "\n",
    "Since we determined that the decision tree in the previous example was not good at classifying bad loan applications, we're going to use the same loan applications' encoded dataset to predict bad loan applications using a random forest."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "17.8.2 Predict Loan Applications\n",
    "\n",
    "Jill now asks you to run a random forest model to make classifications. As you have done before, the first step is to prepare the data for the random forest classifier model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "When we imported our dependencies to create the decision tree in the previous example, we use the \"tree\" module from the sklearn library, from sklearn import tree.\n",
    "\n",
    "For the random forest model, we'll use the \"ensemble\" module from the sklearn library. All the remaining dependencies will be the same. In the dependencies, replace from sklearn import tree with from sklearn.ensemble import RandomForestClassifier so that our dependencies look like the following.\n",
    "\n",
    "# Initial imports.\n",
    "import pandas as pd\n",
    "from path import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next, read in your loans_data_encoded.csv file from previous exercises.\n",
    "\n",
    "# Loading data\n",
    "file_path = Path(\"../Resources/loans_data_encoded.csv\")\n",
    "df_loans = pd.read_csv(file_path)\n",
    "df_loans.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "After the data has been loaded, we're going to preprocess data just like we did for the decision tree model.\n",
    "\n",
    "- Define the features and target\n",
    "- split into training and testing sets\n",
    "- create a StandardScaler instance \n",
    "- Fir the Standard Scaler\n",
    "- Scale the data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Preprocess the Data\n",
    "\n",
    "Now, we're going to walk through the preprocessing steps for the loan applications' encoded data so that we can fit our training and testing sets with the random forest model.\n",
    "\n",
    "If you do not quite remember the steps for preprocessing, add the blocks of code in your Jupyter Notebook as follows.\n",
    "\n",
    "First, we define the features set.\n",
    "\n",
    "# Define the features set.\n",
    "X = df_loans.copy()\n",
    "X = X.drop(\"bad\", axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next, we define the target set. Here, we're using the ravel() method, which performs the same procedure on our target set data as the values attribute.\n",
    "\n",
    "# Define the target set.\n",
    "y = df_loans[\"bad\"].ravel()\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now, we split into the training and testing sets.\n",
    "\n",
    "# Splitting into Train and Test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lastly, we can create the StandardScaler instance, fit the scaler with the training set, and scale the data.\n",
    "\n",
    "# Creating a StandardScaler instance.\n",
    "scaler = StandardScaler()\n",
    "# Fitting the Standard Scaler with the training data.\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scaling the data.\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "17.8.3 Fit the Model, Make Predictions, and Evaluate Results\n",
    "\n",
    "Now that you have prepared the data, you will put the random forest classifier model to practice, then evaluate the results.\n",
    "\n",
    "Now that we have preprocessed the data into training and testing data for both features and target sets, we can fit the random forest model, make predictions, and evaluate the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Fit the Random Forest Model\n",
    "\n",
    "Before we fit the random forest model to our X_train_scaledand y_train training data, we'll create a random forest instance using the random forest classifier, RandomForestClassifier().\n",
    "\n",
    "# Create a random forest classifier.\n",
    "rf_model = RandomForestClassifier(n_estimators=128, random_state=78) \n",
    "\n",
    "The RandomForestClassifier takes a variety of parameters, but for our purposes we only need the n_estimators and the random_state.\n",
    "\n",
    "Consult the sklearn documentation for additional information about the RandomForestClassifier and the parameters it takes.\n",
    "\n",
    "The n_estimators will allow us to set the number of trees that will be created by the algorithm. Generally, the higher number makes the predictions stronger and more stable, but can slow down the output because of the higher training time allocated. The best practice is to use between 64 and 128 random forests, though higher numbers are quite common despite the higher training time. For our purposes, we'll create 128 random forests.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "After we create the random forest instance, we need to fit the model with our training sets.\n",
    "\n",
    "# Fitting the model\n",
    "rf_model = rf_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Make Predictions Using the Testing Data\n",
    "\n",
    "After fitting the model, we can run the following code to make predictions using the scaled testing data:\n",
    "\n",
    "# Making predictions using the testing data.\n",
    "predictions = rf_model.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "The output will be similar as when the predictions were determined for the decision tree."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Evaluate the Model\n",
    "\n",
    "After making predictions on the scaled testing data, we analyze how well our random forest model classifies loan applications by using the confusion_matrix.\n",
    "\n",
    "# Calculating the confusion matrix.\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix.\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Based on the results in the confusion matrix DataFrame, how many true negatives are there? That is, how many were correctly predicted to belong to category 1?\n",
    "\n",
    "18"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "These results are relatively the same as the decision tree model. To improve our predictions, let's increase the n_estimators to 500. After running all the code again, after changing the n_estimators to 500, our confusion matrix DataFrame is about the same as before.\n",
    "\n",
    "Using the equation (TP + TN) / Total, we can determine our accuracy (determine how often the classifier predicts correctly) by running the following code. For this model, our accuracy score is 0.520:\n",
    "\n",
    "# Calculating the accuracy score.\n",
    "acc_score = accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lastly, we can print out the above results along with the classification report for the two classes:\n",
    "\n",
    "# Displaying results\n",
    "print(\"Confusion Matrix\")\n",
    "display(cm_df)\n",
    "print(f\"Accuracy Score : {acc_score}\")\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "From the confusion matrix results, the precision for the the bad loan applications is low, indicating a large number of false positives, which indicates an unreliable positive classification. The recall is also low for the bad loan applications, which is indicative of a large number of false negatives. The F1 score is also low (33).\n",
    "\n",
    "In summary, this random forest model is not good at classifying fraudulent loan applications because the model's accuracy, 0.520, and F1 score are low.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Rank the Importance of Features\n",
    "\n",
    "One nice byproduct of the random forest algorithm is to rank the features by their importance, which allows us to see which features have the most impact on the decision.\n",
    "\n",
    "To calculate the feature importance, we can use thefeature_importances_attribute with the following code:\n",
    "\n",
    "# Calculate feature importance in the Random Forest model.\n",
    "importances = rf_model.feature_importances_\n",
    "importances\n",
    "\n",
    "The output from this code returns an array of scores for the features in the X_test set, whose sum equals 1.0:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To sort the features by their importance with the column in the X_test set, we can modify our code above as follows:\n",
    "\n",
    "# We can sort the features by their importance.\n",
    "sorted(zip(rf_model.feature_importances_, X.columns), reverse=True)\n",
    "\n",
    "In the code, the sorted function will sort the zipped list of features with their column name (X.columns) in reverse order—more important features first—with reverse=True.\n",
    "\n",
    "Drop some of the lower ranked features, like education and/or gender. Does dropping these features improve our random forest model?\n",
    "no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

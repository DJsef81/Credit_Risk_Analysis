{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "17.4.1 Assess Accuracy, Precision, and Sensitivity\n",
    "\n",
    "It's not enough to use a machine learning model to create predictions. The model must answer an important question: how well does it perform? You have seen that accuracy score is one way of assessing a classification model's performance. That is, what percentage of predictions does it get right?\n",
    "\n",
    "Jill explains that there are other ways to validate a classification model, and asks you to look into them. This is where the statistical rubber meets the road!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Accuracy\n",
    "\n",
    "In an earlier module, you learned that the performance of a linear regression model is measured based on the difference between its predicted values and actual values.\n",
    "\n",
    "However, this method cannot be used when the target values are not continuous. Different methods must be used to assess a model with discrete outcomes. We have already seen one way of validating such a model's performance: its accuracy score. An accuracy score is not always an appropriate or a meaningful performance metric, however.\n",
    "\n",
    "Imagine the following scenario, in which a credit card company wishes to detect fraudulent transactions in real time. Historically, out of 100,000 transactions, 10 have been fraudulent. An analyst writes a program to detect fraudulent transactions, but due to an uncaught bug, it flags every transaction as not fraudulent. Out of 100,000 transactions, it correctly classifies the 99,990 transactions that are not fraudulent, and it erroneously classifies all 10 transactions that are fraudulent.\n",
    "\n",
    "What is the accuracy score of the program discussed in the preceding paragraph?\n",
    "0.9999 or 99.99%\n",
    "\n",
    "Since 10 transactions out of 100,000 are fraudulent, 99,990 are legitimate, and 99,990 out of 100,000 is 0.9999.\n",
    "\n",
    "The program's accuracy score appears to be impressive at 99.99%. However, it fails spectacularly at its job, detecting 0 out of 10 fraudulent transactions, a success rate of 0%.\n",
    "\n",
    "Predictions in such a scenario with binary outcomes can be categorized according to the table below:\n",
    "\n",
    "\t           Predicted True\tPredicted False\n",
    "Actually True\tTRUE POSITIVE\tFALSE NEGATIVE\n",
    "Actually False\tFALSE POSITIVE\tTRUE NEGATIVE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Any given prediction falls under one of two categories: true or false. In the context of fraud detection, a true prediction would mean that the model categorizes the transaction as fraudulent. A false prediction means that the model categorizes the transaction as not fraudulent.\n",
    "\n",
    "If a transaction is predicted to be fraudulent and is really a fraudulent transaction, it is a true positive (TP).\n",
    "\n",
    "If a transaction is predicted to be fraudulent but is not fraudulent, it is a false positive (FP). It falsely categorized an innocent transaction as fraudulent.\n",
    "\n",
    "Similarly, if a transaction is predicted to be non-fraudulent but is actually fraudulent, it is a false negative (FN).\n",
    "\n",
    "And when a transaction is predicted to be non-fraudulent and is in reality non-fraudulent, it is a true negative (TN).\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question 1\n",
    "A patient has a streptococcal infection, and a clinical test for the infection came back negative. What is the classification?\n",
    "\n",
    "False negative: The patient has the infection, but the test says otherwise. \n",
    "\n",
    "Question 2\n",
    "A 41-year-old woman takes a mammogram, which comes back positive for breast cancer. Subsequent examination of her breast tissue by a pathologist reveals that her tissue is noncancerous. What is the classification?\n",
    "\n",
    "False positive: The screening test falsely diagnosed breast cancer when the patient did not actually have cancer. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Precision\n",
    "\n",
    "Imagine that a man is experiencing weight loss and chills. He consults an online test that uses machine learning algorithms to see whether he might have cancer, which informs him that he indeed has cancer. However, the online test is not perfect. When the test was previously evaluated in a study, the results were collated into the following table, called a confusion matrix.\n",
    "\n",
    "\t          Predicted True\tPredicted False\n",
    "Actually True\t    30\t              10\n",
    "Actually False\t    20\t              40\n",
    "\n",
    "How many people, in total, were assessed in this study? 100\n",
    "How many people actually had cancer? 30 + 10 = 40 are Actually True.\n",
    "\n",
    "How many people were diagnosed with cancer? 30 + 20 = 50 were Predicted True based on the online test. \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The man in this scenario has a positive diagnosis for cancer. He wants to know how likely it is that he actually has cancer. \n",
    "\n",
    "Precision, also known as positive predictive value (PPV), is a measure of this. Precision is obtained by dividing the number of true positives (TP) by the number of all positives (i.e., the sum of true positives and false positives, or TP + FP).\n",
    "\n",
    "Precision = TP/(TP + FP)\n",
    "\n",
    "How many true positives are there in the study? 30\n",
    "How many false postives are there in the study? 20\n",
    "\n",
    "What is the precision of this test? 30/(30+20) = ⅗ = 0.6\n",
    "\n",
    "In this study, a total of 50 people were predicted to have cancer. Of the 50, 30 people actually had cancer. The precision is therefore 30/50, or 0.6.\n",
    "\n",
    "The terms precision and positive predictive value (PPV) are interchangeable.\n",
    "\n",
    "To summarize, in machine learning, precision is a measure of how reliable a positive classification is. The following formulation may help you in remembering precision: \"I know that the test for cancer came back positive. How likely is it that I have cancer?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sensitivity\n",
    "\n",
    "Another way to assess a model's performance is with sensitivity, also called recall. While the term recall is more commonly used in machine learning, the two terms are synonymous and will be used interchangeably from this point.\n",
    "\n",
    "The following formulation may help you understand sensitivity: \"I know that I have cancer. How likely is it that the test will diagnose it?\" Here is the formula for sensitivity:\n",
    "\n",
    "Sensitivity = TP/(TP + FN)\n",
    "\n",
    "In this context, all who have cancer means true positives (those who have cancer and were correctly diagnosed) and false negatives (those who have cancer and were incorrectly diagnosed as not having cancer). Sensitivity is a measure of how many people who actually have cancer were correctly diagnosed.\n",
    "\n",
    "What is the sensitivity of this test for those who actually had cancer? 30/(30 + 10) = 30/40 = 0.75\n",
    "\n",
    "Which is more important in a screening test to detect cancer: precision or sensitivity?\n",
    "\n",
    "Sensitivity is more important. A test with high sensitivity means few false negatives, though there may be a high number of false positives. In this context, false positives are preferable to false negatives. It’s better to rule out false positive diagnoses than to miss patients who actually have cancer."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Tradeoff Between Precision and Sensitivity\n",
    "\n",
    "There are situations in which high sensitivity is more important. With cancer screening, for example, a high sensitivity is more important than high precision. Remember that high sensitivity means that among people who actually have cancer, most of them will be diagnosed correctly. High precision, on the other hand, means that if the test comes back positive, there's a high likelihood that the patient has cancer.\n",
    "\n",
    "It may appear at first that the two terms refer to the same thing, but they do not. As an extreme example, let's say that among 100 people, 50 have cancer and 50 do not. A screening algorithm is extremely aggressive and labels everyone to have cancer. Since everyone who actually has cancer is detected, the sensitivity is 1.0, or 100%. However, the precision is low: being diagnosed with cancer in this case only means a 50% likelihood of actually having cancer. In other words, there are many false positives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To return to a previous point: Why is high sensitivity more important than precision for a cancer screening test? It's better to detect everyone who might have cancer, even if it means a certain number of false positives, than to miss people who do have cancer. After all, those with a positive result for cancer can undergo further testing to confirm or rule out cancer. The false positives in a highly sensitive test are accepted as a cost of doing business.\n",
    "\n",
    "In contrast, there are situations in which precision is more important than sensitivity. Imagine that the criminal justice system depended on a machine learning algorithm to judge the innocence or guilt of a person on trial. Is high sensitivity or high precision preferable? Perfect sensitivity would mean that everyone who committed a crime is declared guilty. However, a potential consequence of such an aggressive algorithm is that people who didn't commit a crime are also declared guilty. Perfect precision would mean that someone who's been declared guilty actually is guilty. But it also means that there may be people who committed a crime who aren't found guilty. If the justice system values sparing innocent people false imprisonments more than punishing all guilty criminals, precision trumps sensitivity."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "When using a machine learning algorithm to detect fraudulent credit card transactions, which is more important between sensitivity and precision?\n",
    "\n",
    "Sensitivity: False positives can be ruled out by calling the credit card holder. It’s more important to detect potentially fraudulent transactions, so sensitivity is more important here.\n",
    "\n",
    "A political campaign has a database of potential donors. The role of the phone bank is to call potential donors for contributions. Due to limited time and staffing, however, not everyone on the database can be telephoned, and a machine learning algorithm is used to sort the list into likely and unlikely donors. Is sensitivity or precision more important in this case?\n",
    "\n",
    "Precision: In this scenario, because of the limited resources, it’s  more important that predicted donors are likely to be donors than to pick up all potential donors.\n",
    "\n",
    "In an algorithm for spam email detection, which is more important: precision or sensitivity?\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In summary, there's a fundamental tension between precision and sensitivity. Highly sensitive tests and algorithms tend to be aggressive, as they do a good job of detecting the intended targets, but also risk resulting in a number of false positives. High precision, on the other hand, is usually the result of a conservative process, so that predicted positives are likely true positives; but a number of other true positives may not be predicted. In practice, there is a trade-off between sensitivity and precision that requires a balancing act between the two."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "F1 Score\n",
    "The F1 score, also called the harmonic mean, can be characterized as a single summary statistic of precision and sensitivity. The formula for the F1 score is the following:\n",
    "\n",
    "2(Precision * Sensitivity)/(Precision + Sensitivity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To illustrate the F1 score, let's return to the scenario of a faulty algorithm for detecting fraudulent credit card transactions. Say that 100 transactions out of 100,000 are fraudulent.\n",
    "\n",
    "If a faulty algorithm labels every transaction as fraudulent, what is the sensitivity? - 1\n",
    "\n",
    "Using the same scenario above, what is the precision?\n",
    "TP/(TP+FP) = 100/100000 = 0.001"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In such a scenario, the sensitivity is very high, while the precision is very low. Clearly, this is not a useful algorithm. Nor does averaging the sensitivity and precision yield a useful figure. Let's try calculating the F1 score.\n",
    "\n",
    "Using the same scenario above, what is the F1 score in this case?\n",
    "2(Precision * Sensitivity)/(Precision + Sensitivity) = 2( 0.001 * 1.0)/(1.001) = 0.002/1.001 = 0.002"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The F1 score is 0.002. We noted previously that there's usually a trade-off between sensitivity and precision, and that a balance must be struck between the two. \n",
    "\n",
    "A useful way to think about the F1 score is that a pronounced imbalance between sensitivity and precision will yield a low F1 score."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "17.4.2 Confusion Matrix in Practice\n",
    "\n",
    "Let's look at an example of generating a confusion matrix in Python and interpreting it. To get started, download the following file: precision_recall\n",
    "\n",
    "You now understand how precision, recall (sensitivity), and the F1 score can be used to assess a model's performance. Let's return to the Pima Indian diabetes dataset to go through an example in Python. Run all the cells in the notebook. All the data preparation steps have been performed, and a logistic regression model was trained and created predictions.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the next cell, import the relevant modules for validation and print the confusion_matrix, which is the table of true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "print(matrix)\n",
    "\n",
    "The table printed in the notebook is unlabeled, but can be interpreted as the following:\n",
    "\n",
    "                Predicted True\tPredicted False\n",
    "Actually True\t           113\t          12\n",
    "Actually False\t            31\t          36\n",
    "\n",
    "How many true positives are there? - 113\n",
    "How many false positives are there? - 31\n",
    "What is the sensitivity/recall of this model? - 113/(113+12) = 0.9. \n",
    "What is the precision of this model, to two decimal places? - 113/(113+31) = 0.78."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Although we can manually calculate the metrics of the model, Scikit-learn's classification_report module performs the task for us:\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "precision    recall  f1-score   support\n",
    "\n",
    "           0       0.78      0.90      0.84       125\n",
    "           1       0.75      0.54      0.63        67\n",
    "\n",
    "    accuracy                           0.78       192\n",
    "   macro avg       0.77      0.72      0.73       192\n",
    "weighted avg       0.77      0.78      0.77       192\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The precision for prediction of the nondiabetics and diabetics are in line with each other. However, the recall (sensitivity) for predicting diabetes is much lower than it is for predicting an absence of diabetes. The lower recall for diabetics is reflected in the dropped F1 score as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

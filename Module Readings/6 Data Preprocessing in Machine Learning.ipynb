{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "17.6.1 Encode Labels With Pandas\n",
    "\n",
    "One of the tasks involved in data preparation for machine learning is to convert textual data into numerical data.\n",
    "While many datasets contain categorical features (e.g., M or F), machine learning algorithms typically only work with numerical data. Categorical and text data must therefore be converted to numerical data for use in machine learning—which is what we'll do in this section.\n",
    "\n",
    "First, download the files you'll need for this task.\n",
    "\n",
    "Download the files and open the Jupyter Notebook. We'll first import the modules we'll use and open the dataset in a Pandas DataFrame with the following code:\n",
    "\n",
    "import pandas as pd\n",
    "from path import Path\n",
    "\n",
    "file_path = Path(\"../Resources/loans_data.csv\")\n",
    "loans_df = pd.read_csv(file_path)\n",
    "loans_df.head()\n",
    "\n",
    "A preview of the DataFrame reveals seven columns: six features and a target. The dataset contains simulated loan data. There are 500 records, and each row represents a loan application:\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The dataset includes the following columns:\n",
    "\n",
    "- Amount: The loan amount in U.S. dollars.\n",
    "- Term: The loan term in months.\n",
    "- Month: The month of the year when the loan was requested.\n",
    "- Age: Age of the loan applicant.\n",
    "- Education: Educational level of the loan applicant.\n",
    "- Gender: The sex of the loan applicant.\n",
    "- Bad: Status of the application (1: bad, or denial; 0: good, or approval). <- our y target \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Scikit-learn's algorithms only understand numeric data.\n",
    "\n",
    "To use Scikit-learn's machine learning algorithms, the text features (month, education, and gender) will have to be converted into numbers. This process is called encoding. Furthermore, the steps taken to prepare the data to make them usable for building machine learning models are called preprocessing. Encoding text labels into numerical values is one preprocessing step. Later we'll discuss scaling, another preprocessing step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The first and the simplest encoding we'll perform in this dataset is with the gender column, which contains only two values: male and female. We'll convert these values into numerical ones with the pd.get_dummies() method:\n",
    "\n",
    "loans_binary_encoded = pd.get_dummies(loans_df, columns=[\"gender\"])\n",
    "loans_binary_encoded.head()\n",
    "\n",
    "\n",
    "The method takes two arguments:\n",
    "\n",
    "- The first argument for pd.get_dummies() here is the DataFrame.\n",
    "- The second argument specifies the column to be encoded.\n",
    "\n",
    "The gender column has split into two columns, gender_female and gender_male, with each column now containing 0 (false) or 1 (true). Since the first row represents a male loan applicant, the gender_female column reads 0 and the gender_male column reads 1."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It's also possible to encode multiple columns at the same time.\n",
    "\n",
    "loans_binary_encoded = pd.get_dummies(loans_df, columns=[\"education\", \"gender\"])\n",
    "loans_binary_encoded.head()\n",
    "\n",
    "As before, the gender column has split into two columns. The education column has split into four columns (Bachelor, High School or Below, Master or Above, and college), with an associated 0 or 1. \n",
    "\n",
    "If a loan applicant has a bachelor's degree, that column will read 1, and the others (High School or Below, Master or Above, and college) will read 0.\n",
    "\n",
    "For an applicant who did not graduate from high school, the education_Bachelor, education_Master or Above, and education_college columns will be 0, and the education_High School or Below will show 1."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "17.6.2 Encode Labels With Scikit-learn\n",
    "\n",
    "Pandas, as you have seen, offers tools to encode your data. Scikit-learn offers another wayto encode your labels.\n",
    "\n",
    "Scikit-learn's LabelEncoder module can also transform text into numerical data. Let's look at an example. Continue down the notebook from the preceding section:\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df2 = loans_df.copy()\n",
    "df2['education'] = le.fit_transform(df2['education'])\n",
    "\n",
    "The code includes the following elements:\n",
    "\n",
    "- After importing the module, an instance of the label encoder object is created and assigned the variable le.\n",
    "\n",
    "- A copy of the original loans_df is created for this example, but this step is not necessary for using label encoder.\n",
    "\n",
    "- The label encoder's fit_transform() method is used to first train the label encoder, then convert the text data into numerical data.\n",
    "\n",
    "The result is a numerical encoding of the education column. In contrast to pd.get_dummies(), the label encoder assigns a number between 0 and 3 for each of the education categories. The applicant in the first row, for example, has the value 1, which represents high school or below:\n",
    "\n",
    "SKILL DRILL\n",
    "Use Scikit-learn's LabelEncoder to encode the gender column."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "17.6.3 Create Custom Encoding\n",
    "\n",
    "Jill explains that not every machine learning task can be performed by out-of-the-box solutions, meaning libraries written by other programmers. Sometimes you have to roll up your sleeves and write your own custom code!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It's also possible to create custom encoding functions. To understand why this might be useful, let's first look at using the LabelEncoder module.\n",
    "\n",
    "With it, you'll transform the month column into numbers. The goal is to transform each month into its corresponding order: for example, January should be transformed to 1, since it's the first month of the year. Similarly, July should be transformed to 7, since it's the seventh month of the year:\n",
    "\n",
    "# Creating an instance of label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "loans_df[\"month_le\"] = label_encoder.fit_transform(loans_df[\"month\"])\n",
    "loans_df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note that a new instance of LabelEncoder was created here as label_encoder. The month of August, for example, is converted to 1 instead of 8. July is converted to 5 instead of 7.\n",
    "\n",
    "Instead, we can create a dictionary of the months of the year and apply a custom function to convert the month names to their corresponding integers:\n",
    "\n",
    "months_num = {\n",
    "   \"January\": 1,\n",
    "   \"February\": 2,\n",
    "   \"March\": 3,\n",
    "   \"April\": 4,\n",
    "   \"May\": 5,\n",
    "   \"June\": 6,\n",
    "   \"July\": 7,\n",
    "   \"August\": 8,\n",
    "   \"September\": 9,\n",
    "   \"October\": 10,\n",
    "   \"November\": 11,\n",
    "   \"December\": 12,\n",
    "}\n",
    "\n",
    "In the next cell, a lambda function is applied to the month column to perform the actual conversion:\n",
    "\n",
    "loans_df[\"month_num\"] = loans_df[\"month\"].apply(lambda x: months_num[x])\n",
    "The following actions are taking place:\n",
    "\n",
    "- A transformation is made to the values of the month column, and the transformed values are placed in the month_num column.\n",
    "\n",
    "- The apply() method runs the function inside its parentheses on each element of the month column.\n",
    "\n",
    "- The lambda function takes an argument (x), and returns months_num[x]. For example, if the value in the month column is \"June,\" the function returns months_num[\"June\"], which is 6.\n",
    "\n",
    "The DataFrame's month_num column now displays each month as a number:\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The code in the next cell is merely cleanup—it drops the unnecessary columns related to the month:\n",
    "\n",
    "loans_df = loans_df.drop([\"month\", \"month_le\"], axis=1)\n",
    "loans_df.head()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SKILL DRILL\n",
    "Create a new Jupyter Notebook and open loans_data.csv as a Pandas DataFrame. Encode the following labels of the dataset: month, education, and gender. Then save your DataFrame as loans_data_encoded.csv.\n",
    "\n",
    "see 17.6.3 Skill Drill.imbyb file for answer "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "17.6.4 Scale and Normalize Data\n",
    "\n",
    "Data scaling and normalization are steps that are sometimes necessary when preparing data for machine learning. Jill explains that when features have different scales, features using larger numbers can have a disproportionate impact on the model. It is therefore important to understand when to scale and normalize data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Earlier, we worked with a dataset that had already been scaled for us: the values in each column were rescaled to be between 0 and 1. Such scaling is often necessary with models that are sensitive to large numerical values. This is normally the case with models that measure distances between data points. SVM is one model that usually benefits from scaling.\n",
    "\n",
    "In this section, we'll use Scikit-learn's StandardScaler module to scale data. \n",
    "\n",
    "The model -> fit -> predict/transform workflow is also used when scaling data. The standard scaler standardizes the data. This means that each feature will be rescaled so that its mean is 0 and its standard deviation is 1.\n",
    "\n",
    "Get started by downloading the files you'll need. scale.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Open Jupyter Notebook and read in your saved loans_data_encoded.csv file that you created in the previous Skill Drill. If you had trouble creating the encoded data, the dataset is included in the files you downloaded:\n",
    "\n",
    "import pandas as pd\n",
    "from path import Path\n",
    "\n",
    "file_path = Path(\"../Resources/loans_data_encoded.csv\")\n",
    "encoded_df = pd.read_csv(file_path)\n",
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To scale the data in this DataFrame, we'll first import the StandardScaler module and create an instance of it as data_scaler:\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "data_scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The next step is to train the scaler and transform the data. Notice that the fit and transform steps are combined into a single fit_transform() method. However, they can also be used sequentially with data_scaler.fit(), then data_scaler.transform():\n",
    "\n",
    "loans_data_scaled = data_scaler.fit_transform(encoded_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "After transforming the data, we can preview the scaled data:\n",
    "\n",
    "loans_data_scaled[:5]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "array([[ 0.49337687,  0.89789115,  2.28404253, -0.81649658, -0.16890147,\n",
    "        -0.39336295,  1.17997648, -0.08980265, -0.88640526, -0.42665337,\n",
    "         0.42665337],\n",
    "       [ 0.49337687,  0.89789115,  3.10658738, -0.81649658,  0.12951102,\n",
    "         2.54218146, -0.84747452, -0.08980265, -0.88640526,  2.34382305,\n",
    "        -2.34382305],\n",
    "       [ 0.49337687,  0.89789115,  0.3099349 , -0.81649658,  0.42792352,\n",
    "         2.54218146, -0.84747452, -0.08980265, -0.88640526,  2.34382305,\n",
    "        -2.34382305],\n",
    "       [ 0.49337687, -0.97897162, -0.67711892, -0.81649658,  0.72633602,\n",
    "        -0.39336295, -0.84747452, -0.08980265,  1.12815215, -0.42665337,\n",
    "         0.42665337],\n",
    "       [ 0.49337687,  0.89789115, -0.51260995, -0.81649658,  1.02474851,\n",
    "        -0.39336295, -0.84747452, -0.08980265,  1.12815215,  2.34382305,\n",
    "        -2.34382305]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "After standardization, what are the mean and the standard deviation of the first column?\n",
    "Mean: 0, standard deviation: 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "After transformation, the data in each column should be standardized. Let's verify that the mean of each column is 0 and its standard deviation is 1:\n",
    "\n",
    "import numpy as np\n",
    "print(np.mean(loans_data_scaled[:,0]))\n",
    "print(np.std(loans_data_scaled[:,0]))\n",
    "\n",
    "\n",
    "-3.552713678800501e-16\n",
    "0.9999999999999999"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The following actions are taking place:\n",
    "\n",
    "1. The np.mean() and np.std() methods return the mean and standard deviation of an array.\n",
    "loans_data_scaled[:,0] returns all rows and the first column of the dataset.\n",
    "\n",
    "2. The mean of the first column is evaluated as -3.552713678800501e-16, an infinitesimally small number that approximates 0.\n",
    "\n",
    "3. The standard deviation is evaluated as 0.9999999999999999, which is very close to 1.\n",
    "\n",
    "So the standardization was indeed successful, and all numerical columns should now have a mean of 0 and a standard deviation of 1, reducing the likelihood that large values will unduly influence the model."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SKILL DRILL \n",
    "\n",
    "loans_data_scaled.shape returns (500, 11), indicating that there are 500 rows and 11 columns. Create a for loop to verify that all columns of the dataset have been standardized."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

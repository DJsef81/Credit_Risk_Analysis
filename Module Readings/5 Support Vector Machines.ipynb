{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "17.5.1 Overview of Support Vector Machines\n",
    "\n",
    "Now that you're becoming comfortable with using logistic regression and evaluating its results, Jill suggests that you learn about another powerful classification model: support vector machines. Although the name is possibly a little intimidating, you'll be able to bring much of your previous knowledge into using a support vector machine in practice."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Support vector machine (SVM), like logistic regression, is a binary classifier: It can categorize samples into one of two categories (for example, yes or no).\n",
    "\n",
    "To understand support vector machines, let's revisit logistic regression first. A logistic regression model evaluates the probability of an occurrence. For example, the model would take features into account (for example, an applicant's income and credit score) and decide whether to approve the application.\n",
    "\n",
    "The outcome is binary because the only possible options are to approve or to deny the loan application: If the probability is higher than 0.5, the application is classified as approved, or if the probability is less than that, the application is classified as denied. There is a strict cutoff line that divides one classification from the other:\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SVM also categorizes the target variable into one of two classes (for example, approved or denied). However, it differs from logistic regression in several ways. As a linear classifier, the goal of SVM is to find a line that separates the data into two classes:\n",
    "\n",
    "However, there may be many different ways to draw the boundary line, as shown in the diagram below. Which boundary to choose isn't always clear from visual inspection, and choosing the wrong boundary can affect the performance of the model:\n",
    "\n",
    "In a two-dimensional grid, as shown below, SVM draws a line at the edge of each class, and attempts to maximize the distance between them. It does so by separating the data points with the largest possible margins:\n",
    "\n",
    "A hyperplane is the line exactly between the two margins (i.e., equidistant from both margins). Again, the SVM's goal is to find the hyperplane with the widest possible margins (i.e., the largest margin of separation between the two classes):\n",
    "\n",
    "Support vectors are defined as the data points closest to the hyperplane:\n",
    "\n",
    "Real-life data, however, can be messy and will often not yield such a clean line of separation. Imagine that a data point belonging to the blue class were found closer to the cluster of data points that belong to the red class. In this case, would the hyperplane have to be relocated? Would the support vectors have to be redefined?\n",
    "\n",
    "SVMs can accommodate such outliers by using soft margins. A soft margin allows SVM to make allowances for outliers that cross the hyperplane while maintaining support vectors and hyperplane to maximize the overall separation of the two classes:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Up to this point, we have visualized using SVM in datasets with two features. A dataset with three features (e.g., age, education, income) and a target with two classes (e.g., approval or denial of a loan application) would be visualized as a 3D space, with a hyperplane separating the two classes:\n",
    "\n",
    "To summarize, SVM works by separating the two classes in a dataset with the widest possible margins. The margins, however, are soft and can make exceptions for outliers. This stands in contrast to the logistic regression model. In logistic regression, any data point whose probability of belonging to one class exceeds the cutoff point belongs to that class; all other data points belong to the other class."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "17.5.2 SVM in Practice\n",
    "\n",
    "Although the ideas behind support vector machines are different from those behind logistic regression, actually implementing a SVM model is very similar to what you have done. As before, you will split your dataset, create and train a model, create predictions, then validate the model."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that we have looked at how an SVM model works, let's look at using SVM in practice. To get started, download the following files: svm\n",
    "\n",
    "Open the notebook and load the dataset:\n",
    "\n",
    "from path import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = Path('../Resources/loans.csv')\n",
    "df = pd.read_csv(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Each row in the dataset represents an application for a loan, and information is available on the applicant's assets, liabilities, income, credit score, and mortgage size. We also have information on whether the application was approved or denied. Here, the target variable is status, and all other columns are features used to predict the loan application status.\n",
    "\n",
    "It's worth noting that the data in this dataset have been normalized. In this case, the data in the numerical features, such as assets and liabilities, have been scaled to be between 0 and 1.\n",
    "\n",
    "We will discuss scaling in greater detail later, but note for now that some models require scaling the data, and that in this dataset, the scaling has been done for you:\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The next two steps should be familiar. We separate the dataset into features (X) and target (y):\n",
    "\n",
    "y = df[\"status\"]\n",
    "X = df.drop(columns=\"status\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We then further split the dataset into training and testing sets. Note that the shape of the training is (75, 5), meaning 75 rows and five columns. It is generally good practice to stratify the data when splitting into training and testing sets, especially when the dataset is small, as is the case here:\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "   y,  random_state=1, stratify=y)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next, we import the SVC module from Scikit-learn, then instantiate it. The kernel specifies the mathematical functions used to separate the classes. The kernel, in this example, identifies the orientation of the hyperplane as linear. However, a number of kernels exist that define nonlinear boundaries:\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We then train the model with fit():\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next, we create predictions with the model:\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "results = pd.DataFrame({\n",
    "   \"Prediction\": y_pred,\n",
    "   \"Actual\": y_test\n",
    "}).reset_index(drop=True)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We assess the accuracy_score of the model, which is 0.6:\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We then generate a confusion_matrix and print the classification report:\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Assess the performance of a logistic regression model, namely the precision, recall, and F1 scores for the approve category. Compare it with the performance of the SVM model. Which model performs better?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

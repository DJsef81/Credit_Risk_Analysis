{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "17.3.1 Overview of Logistic Regression\n",
    "\n",
    "Jill now believes that you are ready to try your hand at solving a classification problem with machine learning. The first model you will use is logistic regression, a popular classification model. \n",
    "\n",
    "She explains that despite its name, logistic regression is actually not a regression model. It is a classification model. With logistic regression, it is possible to try to answer questions such as whether a credit card holder is likely to miss a payment in the next month."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let's walk through an example of building a logistic regression model in Python. First, download the following file: logistics_regression_demo \n",
    "\n",
    "In the previous linear regression example, the model attempted to predict a person’s salary based on that person’s years of work experience. In this model, is salary a categorical or a continuous variable?\n",
    "\n",
    "A continuous variable, which can take on any value, usually in a specified range. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Logistic regression predicts binary outcomes, meaning that there are only two possible outcomes. \n",
    "\n",
    "An example of logistic regression might be to decide, based on personal information, whether to approve a credit card application. Multiple variables, such as an applicant's age and income, are assessed to arrive at one of two answers: to approve or to deny the application.\n",
    "\n",
    "In other words, a logistic regression model analyzes the available data, and when presented with a new sample, mathematically determines its probability of belonging to a class. If the probability is above a certain cutoff point, the sample is assigned to that class. If the probability is less than the cutoff point, the sample is assigned to the other class.\n",
    "\n",
    "In this section, we'll first walk through an example of logistic regression. Then we'll learn how the algorithm works in greater detail.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Practice Logistic Regression\n",
    "\n",
    "Open logistic_regression.ipynb. In the first cell, Matplotlib and Pandas libraries are imported. In the second cell, a synthetic dataset is generated with Scikit-learn's make_blobs module. You do not need to focus on the details; the sole purpose here is to create two clusters of data named X and y:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(centers=2, random_state=42)\n",
    "\n",
    "print(f\"Labels: {y[:10]}\")\n",
    "print(f\"Data: {X[:10]}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The centers argument specifies the number of clusters in the dataset; in this case there are two clusters.\n",
    "\n",
    "The random_state ensures reproducibility of this dataset: even though the numbers in this dataset are generated pseudo-randomly, specifying 42 as the random_state argument will generate the identical dataset in the future.\n",
    "\n",
    "Notice also that the dataset is split into X and y arrays. \n",
    "\n",
    "X contains the coordinates of each data point, and y contains information on the class of each data point. That is, each point belongs to either class 0 or 1. Purple dots belong to class 0, while yellow dots belong to class 1. The term \"labels\" is used here as a synonym for target variable."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the next cell, the dataset is visualized. We see a scatter plot with two clearly separated groups of data points. The data points to the left of 0 on the x-axis are purple, while those to its right are colored yellow. The logistic regression model will be trained on this data, and it will be able to categorize a new data point as one of the two classes (yellow or purple):\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Split the Dataset Into Train and Test Sets\n",
    "\n",
    "In the next cell, we split the dataset into two: train and test datasets. Imagine that you are studying for a major exam and have a test bank of 100 questions available. A good strategy might be to use 75 of the questions as you study for the exam. These questions will help pinpoint important topics and patterns of information you might encounter on the actual exam.\n",
    "\n",
    "Then, a few days before the exam, you sit down to answer the remaining 25 questions in a mock test. Since you haven't seen these questions before, how well you perform on them will give you a good idea of how you will do on the actual exam."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Similarly, a dataset is split into training and testing sets in supervised learning. The model uses the training dataset to learn from it. It then uses the testing dataset to assess its performance. If you use your entire dataset to train the model, you won't know how well the model will perform when it encounters unseen data. That is why it's important to set aside a portion of your dataset to evaluate your model."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the next cell, the dataset is split into training and testing sets:\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "    y, random_state=1, stratify=y)\n",
    "    \n",
    "Recall that X is the input, and y is the output, or what we wish to predict. Scikit-learn's train_test_split module takes X and y as arguments and splits each into training and test sets. So in all, we end up with four sets. X is split into X_train and X_test sets, and y is split into y_train and y_test sets."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The train_test_split() function has four arguments here. The first two, as discussed, are X and y, which are split into train and test sets. \n",
    "\n",
    "We saw previously that random_state is used to make the data reproducible. Specifying a random state of 1 ensures that the same rows are assigned to train and test sets, respectively. \n",
    "\n",
    "A different random state number would distribute different rows of data to the train and test sets. \n",
    "\n",
    "Note that the random_state argument is used here only for pedagogical purposes, so that you will obtain the same results shown here when you run the notebook. You will not need to use it when you create your own analyses."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The last argument, stratify, also deserves discussion. \n",
    "\n",
    "Stratifying a dataset divides it proportionally. \n",
    "\n",
    "For example, say that 60% of a dataset belongs to the yellow class, and 40% of it belongs to the purple class. Stratifying it ensures that when the entire dataset is split into training and testing sets, 60% of both will belong to the yellow class, and 40% will belong to the purple class. \n",
    "\n",
    "Without specifying stratification, samples will be assigned randomly, so that it is possible to end up with a training set with a 68-32 split and a testing set with a 65-35 split. \n",
    "\n",
    "In our current dataset, both the purple and yellow clusters have roughly equal sizes, and a stratified split will not make a significant difference in the outcome."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A dataset has 800 observations of Class A and 200 observations of Class B. Which of the following is a stratified split?\n",
    "\n",
    "Training set: 600 Class A, 200 Class B. Testing set: 150 Class A, 50 Class B.\n",
    "\n",
    "Class A makes up 75% of both training and testing sets. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "However, imagine a scenario in which one class is significantly larger than the other. \n",
    "\n",
    "For example, let's say that we're analyzing 10,000 credit card transactions, and only 40 of them are flagged as fraudulent. We might allocate 75% of the dataset to the training set, and 25% to the testing set, so we'd expect the training set to contain 75% of the fraudulent transactions (30), and the testing set to contain 25% of the fraudulent transactions (10). \n",
    "\n",
    "Without stratification, it's possible for the fraudulent transactions to be distributed disproportionately—for example, 20 to the training set and 20 to the testing set. And as the model trains on the unrepresentative data, it can reach wrong conclusions. It's therefore important to consider stratifying the data, especially when the classes are severely unbalanced, or when the dataset is small.\n",
    "\n",
    "Stratified sampling usually results in a more representative sample than random sampling.\n",
    "\n",
    "Stratification can be especially helpful when the dataset is small.\n",
    "\n",
    "Stratification is especially important when there's a class imbalance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Instantiate a Logistic Regression Model\n",
    "\n",
    "In the next cell, we create a logistic regression model:\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(solver='lbfgs', random_state=1)\n",
    "classifier\n",
    "\n",
    "The above code returns:\n",
    "\n",
    "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "   intercept_scaling=1, 11_ratio=None, max_iter=100,\n",
    "   multi_class='warn', n_jobs=None, penalty='12',\n",
    "   random_state=1, solver='lbfgs' tol=0.0001, verbose=0,\n",
    "   warm_start=False)\n",
    "   \n",
    "Let's break down this code:\n",
    "\n",
    "- We first import LogisticRegression from the Scikit-learn library, and then instantiate the model.\n",
    "\n",
    "- The solver argument is set to 'lbfgs', which is an algorithm for learning and optimization. The particular solver isn't very important in this example, but note that a number of optimizers exist.\n",
    "\n",
    "- Once again, the random_state is specified so that you'll be able to reproduce the same results as you run this notebook.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Train the Logistic Regression Model\n",
    "\n",
    "The next step is to train the model with the training set data. We use the fit() method to train the model:\n",
    "\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Validate the Logistic Regression Model\n",
    "\n",
    "The next step is to create predictions and assemble the results into a Pandas DataFrame:\n",
    "\n",
    "predictions = classifier.predict(X_test)\n",
    "pd.DataFrame({\"Prediction\": predictions, \"Actual\": y_test})\n",
    "\n",
    "Let's break down this code:\n",
    "\n",
    "- The first line of code uses the predict() method to create predictions based on X_test.\n",
    "\n",
    "- The second line creates a DataFrame of predicted values and actual values."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next, we validate the model, or evaluate its performance. You can create a model and use it to make predictions, but you won't know how good the model is unless you assess its performance:\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The model achieved an accuracy score of 1.0. This means that every single observation in the testing set was predicted correctly by the model. \n",
    "\n",
    "All samples belonging to class 1 (yellow) were correctly predicted, and all samples belonging to class 0 (purple) were likewise correctly predicted by the model. \n",
    "\n",
    "Although perfect accuracy was achieved in this example, it is rare in actual practice. Moreover, an extremely high metric should raise your suspicion of overfitting. \n",
    "\n",
    "Overfitting refers to an instance in which the patterns picked up by a model are too specific to a specific dataset. We will discuss overfitting in greater detail later in the module."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The sole purpose of the next cell is to create a new data point, which shows up as a red dot on the new plot. The logistic regression model will then classify this sample as belonging to class 0 (purple) or class 1 (yellow):\n",
    "\n",
    "import numpy as np\n",
    "new_data = np.array([[-2, 6]])\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.scatter(new_data[0, 0], new_data[0, 1], c=\"r\", marker=\"o\", s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Once again, we use predict() to predict which class the new data point belongs to. It turns out, as expected, that the red dot belongs to the purple cluster:\n",
    "\n",
    "predictions = classifier.predict(new_data)\n",
    "print(\"Classes are either 0 (purple) or 1 (yellow)\")\n",
    "print(f\"The new point was classified as: {predictions}\")\n",
    "\n",
    "Returns the following: \n",
    "\n",
    "Classes are either 0 (purple) or 1 (yellow)\n",
    "The new point was classified as: [0]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let's summarize the steps we took to use a logistic regression model:\n",
    "\n",
    "1. Create a model with LogisticRegression().\n",
    "\n",
    "2. Train the model with model.fit().\n",
    "\n",
    "3. Make predictions with model.predict().\n",
    "\n",
    "4. Validate the model with accuracy_score()."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "17.3.2 Logistic Regression to Predict Diabetes\n",
    "\n",
    "Now that you've gotten your feet wet with logistic regression, Jill believes that it's time to implement a model with a real dataset. In the next step, you will follow the familiar pattern as you instantiate a model, train it, create predictions, then validate the model."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Let's solve another classification problem with logistic regression. This time, we'll use a dataset on diabetes among Pima Indian women.\n",
    "\n",
    "First, download the files you'll need.\n",
    "\n",
    "Open diabetes.ipynb. We can see from the preview of the DataFrame that multiple variables (also called features), such as the number of previous pregnancies, blood glucose level, and age, can be used to predict the outcome: whether a person has diabetes (1) or does not have diabetes (0):\n",
    "\n",
    "A common task in machine learning is data preparation. In previous examples, we assigned the label X to input variables, and used them to predict y, or the output. With this diabetes dataset, we need to categorize features from the target. We can do so by separating the Outcome column from the other columns.\n",
    "\n",
    "y = df[\"Outcome\"]\n",
    "X = df.drop(columns=\"Outcome\")\n",
    "\n",
    "\n",
    "The Outcome column is defined as y, or the target.\n",
    "X, or features, is created by dropping the Outcome column from the DataFrame.\n",
    "\n",
    "Later in this module, we'll be splitting the training and testing data, creating a logistic regression model, fitting the training data, and making a prediction. What steps can you do on your own in the following Skill Drills?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let's retrace the steps taken so far. We first split the dataset into training and testing sets:\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "   y, random_state=1, stratify=y)\n",
    "\n",
    "Examining the shape of the training set with X_train.shape returned (576,8), meaning that there are 576 samples (rows) and eight features (columns)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The next step was to create a logistic regression model with the specified arguments for solver, max_iter, and random_state:\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(solver='lbfgs',\n",
    "   max_iter=200,\n",
    "   random_state=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next, we trained the model with the training data:\n",
    "\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To create predictions for y-values, we used the X_test set:\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "When the first 20 rows of the predicted y-values (y_pred) are compared with the actual y-values (y_test), we see that most of the predictions are correct, but that there are also some missed predictions, such as rows 14 and 15:\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The final step is to answer an important question: how well does our logistic regression model predict? We do so with sklearn.metrics.accuracy_score:\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This method compares the actual outcome (y) values from the test set against the model's predicted values. In other words, y_test are the outcomes (whether or not a woman has diabetes) from the original dataset that were set aside for testing. The model's predictions, y_pred, were compared with these actual values (y_test). The accuracy score is simply the percentage of predictions that are correct. In this case, the model's accuracy score was 0.776, meaning that the model was correct 77.6% of the time."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "17.3.3 How Logistic Regression Works\n",
    "\n",
    "Jill informs you that a good data scientist not only understands the hows but the whys. She explains that understanding how a model works helps a data scientist assess a machine learning model's strengths, weaknesses, and how best to use it. She asks you to look into how a logistic regression model works."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Before launching into a discussion of logistic regression, let's quickly review linear regression. The image below shows a scatter plot, through which a best fit line is drawn. In this case, the chart depicts the size and price of houses in a particular district:\n",
    "\n",
    "As the size of a house increases, it will generally fetch a higher price on the market. Each point represents a specific house in that district. How would you best describe the data values in the graph? \n",
    "\n",
    "The data values are continuous."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "But what happens when the outcome variable is binary, meaning that only two outcomes are possible? For example, let's say that the x-axis on the scatter plot below represents a college applicant's score on an entrance exam, and that the y-axis represents acceptance to a particular college. There is a wide range of test scores, but there are only two possible outcomes: acceptance (1) or rejection (0):\n",
    "\n",
    "Linear regression clearly would not work in this case. \n",
    "\n",
    "Instead, the probability of an outcome is represented with the following equation:\n",
    "\n",
    "log(probability of admission/(1 - probability of admission))\n",
    "\n",
    "It's important to note that the results of this equation ultimately generate an S-shaped curve that represents the probability of being admitted at a given test score.\n",
    "\n",
    "Each test score along the x-axis is associated with a probability of acceptance. In the following example, for a student with a score of A, the probability of acceptance is somewhat higher than 50%, whereas a student with a score of B has a nearly 100% chance of being admitted:\n",
    "\n",
    "This S-shaped curve, also called a sigmoid curve, can then be used to predict acceptance for new applicants. The score at which the vertical line is drawn has approximately 80% probability of acceptance. Because this value exceeds the cutoff point, which in this case is defined as 50%, it's predicted that candidates with this score will be accepted:\n",
    "\n",
    "Finally, the results are made linear with a little more mathematical manipulation. The final product is a linear equation, like the one seen in linear regression. It is for this reason that both linear regression and logistic regression are considered linear models."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The company you are working for wants to filter spam emails based on certain criteria. What type of regression analysis is this? \n",
    "- Logistic\n",
    "\n",
    "What type of variable would describe whether an email is spam or not? \n",
    "- Binary"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The Logit Function\n",
    "\n",
    "This brief mathematical discussion of logistic regression is optional. Feel free to skip this section, but keep reading if you'd like a bit more explanation behind the math.\n",
    "\n",
    "We saw earlier that the sigmoid probability curve is generated based on the following equation, which is also called the logit function:\n",
    "\n",
    "log(probability of admission/(1 - probability of admission))\n",
    "The fraction seen here is the ratio of the probability of an occurrence and nonoccurrence. For example, let's say that applicants with a given score have a 90% probability of acceptance. Therefore, they will have a 10% probability of rejection. The logarithm of the ratio of the two probabilities is expressed as log(0.9/0.1). Based on this equation (which undergoes some rearrangement), the S-shaped curve can be created from the existing data points.\n",
    "\n",
    "So why is a logarithm of the ratio obtained? Logarithms are useful when dealing with ratios. If you are unfamiliar or rusty with logarithms, a short description of them is that they are the opposite of exponents, just as subtractions undo additions, and divisions undo multiplications. In the following illustration, to the left is an exponential curve. In this case, as the value on the x-axis increases, its y-axis value increases rapidly. The illustration on the right shows that the curve is been straightened into a line after plotting the logarithms of the values, since logarithms undo exponents:\n",
    "\n",
    "The figure on the left shows an exponential curve. In the figure on\n",
    "the right, the exponential curve has been offset with a logarithm,\n",
    "yielding a straight\n",
    "line.\n",
    "\n",
    "To understand why logarithms might be useful, consider two ratios: an even ratio and an extremely lopsided ratio. A score with an even chance (50%) of acceptance also has an even chance (50%) of being rejected. The ratio of the two probabilities is 0.5/0.5, or 1. On the other hand, an application that has 99.999% chance of being accepted has 0.001% chance of being rejected. The ratio of the two probabilities is 0.99999/0.00001 or 99999. The discrepancy between the two ratios is almost 100,000-fold! The use of logarithms smoothens out this asymmetry by scaling the numbers.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
